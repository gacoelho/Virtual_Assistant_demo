# Virtual Assistant Demo

# üíº Assistente Virtual da Vale ‚Äî Demo Local com Ollama

Este √© um prot√≥tipo de **assistente virtual corporativo** desenvolvido para simular intera√ß√µes internas na empresa **Vale**, com foco em tarefas como:

- Solicita√ß√£o de f√©rias  
- Reembolso de viagens  
- Atualiza√ß√£o de perfil do empregado  
- Consulta de benef√≠cios  

---

## üß† Tecnologias Utilizadas

| Tecnologia  | Fun√ß√£o Principal                                  |
|-------------|---------------------------------------------------|
| [`Ollama`]  |Execu√ß√£o local do modelo LLM                       |
| `gradio`    | Interface web interativa em formato de chat       |
| `rapidfuzz` | Fuzzy matching entre perguntas e base de inten√ß√µes|
| `json`      | Armazenamento da base de dados e logs             |

---

## üõ†Ô∏è Requisitos

- Python 3.8+
- `ollama` instalado e funcional (com o modelo `gemma:2b` puxado)
- Ambiente virtual Python (recomendado)

---

## üöÄ Como Executar

### 1. Instale o [Ollama](https://ollama.com/download)

Siga as instru√ß√µes do site oficial para seu sistema (Linux, macOS, Windows).

### 2. Baixe o modelo `gemma:2b`

```bash
ollama pull gemma:2b

obs: "gemm:2b √© um modelo de linguagme grande '(LLM)' de 2 Bilh√µes de par√¢metros que necessita de pelo menos 3.1 GB de m√©moria RAM para funcionar. Antes de rodar o programa use o comando no termina: ollama run gemma:2b e verifica s√© h√° algum problema com a o requerimento de RAM. Depois de crtl + d para sair do prompt do gemma."

### 3, Baixa o requirement.txt 

pip install -r requirement.txt

### 4, Rode o progrma 

python main.py

'entre no URL gerado pelo Gradio:' http://127.0.0.1:7860


